@misc{pollock_open_2011,
    title = {Open {Data}: a means to an end, not an end in itself},
    shorttitle = {Open {Data}},
    url = {https://blog.okfn.org/2011/09/15/open-data-a-means-to-an-end-not-an-end-in-itself/},
    abstract = {The following is a post by Rufus Pollock, co-Founder of the Open Knowledge Foundation. In almost all the talks I give about open data or content, I aim, at least once, to make the statement along the lines: "Openness for data and content is not an end in itself, it's a means to an end"},
    journal = {Open Knowledge Blog},
    author = {Pollock, Rufus},
    month = sep,
    year = {2011},
}
@article{thompson_laure_building_nodate,
    title = {Building {Large}-{Scale} {Collections} of {Genre} {Fiction}: {Final} {Report}},
    url = {https://laurejt.github.io/papers/htrc-acs-final-report.pdf},
    author = {Thompson, Laure, David, Mimno},
}
@misc{noauthor_princeton_nodate,
    title = {Princeton {Prosody} {Archive}},
    url = {https://prosody.princeton.edu/},
    abstract = {The Princeton Prosody Archive is a full-text searchable database of thousands of historical documents about the study of language and the study of poetry.},
    language = {en},
    urldate = {2025-07-11},
    journal = {Princeton Prosody Archive},
}
@misc{noauthor_welcome_nodate,
    title = {Welcome to {HathiTrust} {\textbar} {HathiTrust} {Digital} {Library}},
    url = {https://www.hathitrust.org/about/},
    urldate = {2025-07-11},
}
@misc{noauthor_keeping_nodate,
    title = {Keeping digitised heritage accessible: the case of broken links},
    shorttitle = {Keeping digitised heritage accessible},
    url = {https://pro.europeana.eu/post/keeping-digitised-heritage-accessible-the-case-of-broken-links},
    abstract = {Ever find yourself frustrated by a link that doesn’t take you to where you want to go? Is there anything worse than a 404 message that the page doesn’t exist…},
    language = {en},
    urldate = {2025-07-07},
    journal = {Europeana PRO},
}
@misc{noauthor_text_nodate,
    title = {Text {Correction}},
    url = {https://trove.nla.gov.au/help/your-trove-tools/text-correction},
    abstract = {Contents How to Edit Text Why Text Needs Editing Find Articles to Edit Editing Guidelines Troubleshooting},
    language = {en},
    urldate = {2025-07-11},
    journal = {Trove},
}
@misc{noauthor_home_nodate,
    title = {Home - {Trove}},
    url = {https://trove.nla.gov.au/},
    urldate = {2025-07-11},
}
@misc{noauthor_page_nodate,
    title = {Page d'accueil {\textbar} {Gallica}},
    url = {https://gallica.bnf.fr/accueil/fr/html/accueil-fr},
    urldate = {2025-07-11},
}
@misc{noauthor_discover_nodate,
    title = {Discover {Europe}’s digital cultural heritage},
    url = {https://www.europeana.eu/en},
    abstract = {Search, save and share art, books, films and music from thousands of cultural institutions},
    language = {en},
    urldate = {2025-07-11},
}
@misc{noauthor_e-rara_nodate,
    title = {e-rara},
    url = {https://www.e-rara.ch/},
    urldate = {2025-07-11},
}
@misc{noauthor_e-manuscripta_nodate,
    title = {e-manuscripta},
    url = {https://www.e-manuscripta.ch/},
    urldate = {2025-07-11},
}
@misc{noauthor_search_nodate,
    title = {Search service {\textbar} {Archives}, libraries, museums {\textbar} {Finna}.fi},
    url = {https://finna.fi/?lng=en-gb},
    language = {en-gb},
    urldate = {2025-07-11},
}
@article{koeser_visualizing_2020,
    title = {Visualizing the {Collections}},
    issn = {2769-3619},
    url = {https://prosody.princeton.edu/editorial/2020/01/visualizing-collections/},
    abstract = {What is the best way to visualize the relative size and overlap of the seven PPA collections? My quest through Venn diagrams, zoomable treemaps, and UpSet plots led me to an experimental alternative.},
    language = {en},
    urldate = {2025-07-10},
    journal = {Princeton Prosody Archive},
    author = {Koeser, Rebecca Sutton},
    month = jan,
    year = {2020},
    note = {Publisher: Center for Digital Humanities, Princeton University},
}
@misc{noauthor_hathitrust_nodate,
    title = {{HathiTrust} {Research} {Center} – {HathiTrust} {Digital} {Library}},
    url = {https://www.hathitrust.org/about/research-center/},
    urldate = {2025-07-11},
}
@article{stevens_new_2017,
    title = {New {Metadata} {Recipes} for {Old} {Cookbooks}: {Creating} and {Analyzing} a {Digital} {Collection} {Using} the {HathiTrust} {Research} {Center} {Portal}},
    issn = {1940-5758},
    shorttitle = {New {Metadata} {Recipes} for {Old} {Cookbooks}},
    url = {https://journal.code4lib.org/articles/12548},
    abstract = {The Early American Cookbooks digital project is a case study in analyzing collections as data using HathiTrust and the HathiTrust Research Center (HTRC) Portal. The purposes of the project are to create a freely available, searchable collection of full-text early American cookbooks within the HathiTrust Digital Library, to offer an overview of the scope and contents of the collection, and to analyze trends and patterns in the metadata and the full text of the collection. The digital project has two basic components: a collection of 1450 full-text cookbooks published in the United States between 1800 and 1920 and a website to present a guide to the collection and the results of the analysis., This article will focus on the workflow for analyzing the metadata and the full-text of the collection. The workflow will cover: 1) creating a searchable public collection of full-text titles within the HathiTrust Digital Library and uploading it to the HTRC Portal, 2) analyzing and visualizing legacy MARC data for the collection using MarcEdit, OpenRefine and Tableau, and 3) using the text analysis tools in the HTRC Portal to look for trends and patterns in the full text of the collection.},
    number = {37},
    urldate = {2025-07-08},
    journal = {The Code4Lib Journal},
    author = {Stevens, Gioia},
    month = jul,
    year = {2017},
    note = {Repository: Code4Lib Journal},
}
@misc{noauthor_recommended_nodate,
    title = {Recommended worksets • {HTRC} {Analytics}},
    url = {https://analytics.hathitrust.org/staticrecommendedworksets},
    urldate = {2025-07-07},
}
@article{parulian_uncovering_2022,
    title = {Uncovering {Black} {Fantastic}: {Piloting} {A} {Word} {Feature} {Analysis} and {Machine} {Learning} {Approach} for {Genre} {Classification}},
    volume = {59},
    copyright = {85th Annual Meeting of the Association for Information Science \& Technology {\textbar} Oct. 29 – Nov. 1, 2022 {\textbar} Pittsburgh, PA. Author(s) retain copyright, but ASIS\&T receives an exclusive publication license.},
    issn = {2373-9231},
    shorttitle = {Uncovering {Black} {Fantastic}},
    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pra2.620},
    doi = {10.1002/pra2.620},
    abstract = {Given the size of digital library collections and the inconsistencies in their genre-related bibliographic metadata, as digital libraries grow and their contents are opened for computational analysis, finding materials of interest becomes a major challenge. This challenge increases for sub-genres and other categories of text data that are less distinct from the whole. This project pilots machine learning methods and word feature analysis for identifying Black Fantastic genre texts within the HathiTrust Digital Library. These texts are sometimes referred to as “Afrofuturism” but more commonly today described as “Black Fantastic,” in which African Diaspora artists and creators engage with the intersections of race and technology in their works with a primary focus on world-building. Black Fantastic texts pose a challenge to genre classification, as they incorporate aspects of science fiction and fantasy with typical characteristics of African Diaspora-produced literature. This paper presents and reports on results from a pilot predictive modeling process to computationally identify Black Fantastic texts using curated word feature sets for each class of data: general English-language fiction, Black-authored fiction, and Black Fantastic fiction.},
    language = {en},
    number = {1},
    urldate = {2025-07-10},
    journal = {Proceedings of the Association for Information Science and Technology},
    author = {Parulian, Nikolaus Nova and Dubnicek, Ryan and Worthey, Glen and Evans, Daniel J. and Walsh, John A. and Downie, J. Stephen},
    year = {2022},
    note = {\_eprint: https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/pra2.620},
    keywords = {digital library, genre classification, hathitrust, machine learning, natural language processing},
    pages = {242--250},
}
@misc{dubnicek_ryan_piloting_2023,
    address = {Graz, Austria},
    title = {Piloting {A} {Machine} {Learning} {Approach} to {Identify} {English}-{Language} {Fiction} in the {HathiTrust} {Digital} {Library}},
    author = {{Dubnicek, Ryan} and {Underwood, Ted}},
    month = jul,
    year = {2023},
}
@article{john_a_walsh_library_nodate,
    title = {“{The} {Library} is {Open}!”: {Open} {Data} and an {Open} {API} for the {HathiTrust} {Digital} {Library}},
    url = {https://ceur-ws.org/Vol-3558/paper7875.pdf},
    author = {{John A. Walsh} and {Glen Layne-Worthey} and {Jacob Jett} and {Boris Capitanu} and {Peter Organisciak} and {Ryan Dubnicek} and {J. Stephen Downie}},
}
@misc{noauthor_constellate_2019,
    title = {Constellate},
    url = {https://labs.jstor.org/projects/text-mining/},
    abstract = {Constellate, a part of ITHAKA's portfolio of non-profit services, is the only text analysis platform that integrates access to scholarly content and open educational resources into a cloud-based lab to help faculty more easily and effectively teach text analysis and data skills.},
    language = {en},
    urldate = {2025-07-10},
    journal = {JSTOR Labs},
    month = dec,
    year = {2019},
}
@misc{noauthor_plans_nodate,
    title = {Plans for the {HathiTrust} {Research} {Center} – {HathiTrust} {Digital} {Library}},
    url = {https://www.hathitrust.org/press-post/plans-for-hathitrust-research-center/},
    urldate = {2025-07-11},
}
@article{hill_quantifying_2019,
    title = {Quantifying the impact of dirty {OCR} on historical text analysis: {Eighteenth} {Century} {Collections} {Online} as a case study},
    volume = {34},
    issn = {2055-7671},
    shorttitle = {Quantifying the impact of dirty {OCR} on historical text analysis},
    url = {https://doi.org/10.1093/llc/fqz024},
    doi = {10.1093/llc/fqz024},
    abstract = {This article aims to quantify the impact optical character recognition (OCR) has on the quantitative analysis of historical documents. Using Eighteenth Century Collections Online as a case study, we first explore and explain the differences between the OCR corpus and its keyed-in counterpart, created by the Text Creation Partnership. We then conduct a series of specific analyses common to the digital humanities: topic modelling, authorship attribution, collocation analysis, and vector space modelling. The article concludes by offering some preliminary thoughts on how these conclusions can be applied to other datasets, by reflecting on the potential for predicting the quality of OCR where no ground-truth exists.},
    number = {4},
    urldate = {2025-04-24},
    journal = {Digital Scholarship in the Humanities},
    author = {Hill, Mark J and Hengchen, Simon},
    month = dec,
    year = {2019},
    pages = {825--843},
}
@misc{naydan_beyond_2024,
    title = {Beyond the {Walled} {Gardens}: {Reinventing} the {Digital} {Research} {Landscape} with the {Princeton} {Prosody} {Archive}},
    shorttitle = {Beyond the {Walled} {Gardens}},
    url = {https://zenodo.org/records/13850968},
    abstract = {This poster uses the rich case study of the Princeton Prosody Archive to show how it is possible – but difficult – to break out of the “walled gardens” that comprise today’s digital research landscape. The poster's visualizations illustrate the innovative technical architecture of the database, surface the invisible labor it takes to build a resource of this kind, and compare the variable levels of access granted by each "garden." 
This poster was accepted and presented at the DH2024 conference in Washington, D.C.},
    language = {eng},
    urldate = {2025-07-09},
    author = {Naydan, Mary and Koeser, Rebecca and Martin, Meredith},
    month = sep,
    year = {2024},
    doi = {10.5281/zenodo.13850968},
}
@book{martin_poetrys_2025,
    edition = {1st ed.},
    title = {Poetry's data : digital humanities and the history of prosody},
    isbn = {978-0-691-25472-2},
    shorttitle = {Poetry's data},
    url = {https://catalog.princeton.edu/catalog/99131423406806421},
    publisher = {Princeton: Princeton University Press},
    author = {Martin, Meredith},
    year = {2025},
}
@misc{noauthor_shakespeare_2020,
    title = {Shakespeare and {Company} {Project}},
    url = {https://shakespeareandco.princeton.edu/},
    abstract = {Recreating the world of the Lost Generation in interwar Paris},
    language = {en},
    urldate = {2025-07-11},
    journal = {Shakespeare and Company Project},
    year = {2020},
    note = {Publisher: Center for Digital Humanities, Princeton University},
}
@article{naydan_book_2024,
    title = {Book {Excerpts}, {Journal} {Articles}, and {Better} {Metadata}},
    issn = {2769-3619},
    url = {https://prosody.princeton.edu/editorial/2024/08/book-excerpts-journal-articles-and-better-metadata/},
    abstract = {How we hand-curated metadata so researchers can search and browse only the relevant parts of larger works.},
    language = {en},
    urldate = {2025-07-10},
    journal = {Princeton Prosody Archive},
    author = {Naydan, Mary and Koeser, Rebecca Sutton},
    editor = {Martin, Meredith},
    month = aug,
    year = {2024},
    note = {Publisher: Center for Digital Humanities, Princeton University},
}
@misc{noauthor_how_nodate,
    title = {How to {Use} {HathiTrust} {Data} {Resources} – {HathiTrust} {Digital} {Library}},
    url = {https://www.hathitrust.org/member-libraries/resources-for-librarians/data-resources/},
    urldate = {2025-07-03},
}
@misc{noauthor_scansion_nodate,
    title = {The {Scansion} of {Middle} {English} {Alliterative} {Verse}},
    url = {https://prosody.princeton.edu/archive/uiug.30112046384886-p58/},
    language = {en},
    urldate = {2025-07-11},
    journal = {Princeton Prosody Archive},
}
@article{emerson_o_f_development_1889,
    title = {The {Development} of {Blank} {Verse}: {A} {Study} of {Surrey}},
    volume = {4},
    shorttitle = {The {Development} of {Blank} {Verse}},
    url = {https://prosody.princeton.edu/archive/mdp.39015060429746-p466/},
    language = {en},
    urldate = {2025-07-11},
    journal = {Modern language notes},
    author = {{Emerson, O. F.}},
    year = {1889},
    pages = {466--472},
}
@article{omond_thomas_stewart_swinburne_1909,
    title = {Swinburne as a {Metrician}},
    volume = {76},
    url = {https://prosody.princeton.edu/archive/uc1.c2641998-p32/},
    language = {en},
    urldate = {2025-07-11},
    journal = {Princeton Prosody Archive},
    author = {{Omond, Thomas Stewart}},
    year = {1909},
    pages = {32--33},
}
@article{thompson_building_nodate,
    title = {Building {Large}-{Scale} {Collections} of {Genre} {Fiction}: {Final} {Report}},
    url = {https://laurejt.github.io/papers/htrc-acs-final-report.pdf},
    author = {Thompson, Laure and {Mimno, David}},
}
@misc{underwood_page-level_2014,
    title = {Page-{Level} {Genre} {Metadata} for {English}-{Language} {Volumes} in {HathiTrust}, 1700-1922},
    abstract = {Page-by-page genre predictions for 854,476 English-language volumes printed between 1700 and 1922, keyed to the texts in HathiTrust Digital Library. This research was supported by the National Endowment for the Humanities and the American Council of Learned Societies.The genre predictions were produced by an ensemble of regularized logistic classifiers, and are intended to support research that explores broad trends in literary history. Since volumes usually contain multiple genres, page-level metadata is necessary to create machine-readable collections in a particular genre.Only very broad categories are discriminated here (fiction, poetry, drama, nonfiction prose, paratext). Overall average accuracy is 93.6\%, but confidence metrics are included that allow researchers to trade recall for enhanced precision. For instance, the filtered subsets of fiction, poetry, and drama (included here as fiction.tar.gz, etc.) have higher than 97\% precision.Predictions are included as JSON objects in separate files, one for each volume. The tar.gz files prefixed with "all" include all 854,476 volumes, divided by date. The tar.gz files named for genres contain subsets of volumes that have been filtered to achieve greater than 97\% precision in that particular genre. Specifically, they include 18,111 vols containing drama, 102,349 vols containing fiction, and 61,286 vols containing poetry. These datasets were filtered both with confidence metrics created by a logistic model and by manual editing. Ringers.csv is a list of volumes that we had to manually remove; scholars who select their own datasets from the larger collection (of files beginning "all") may also want to consider filtering out these tricky cases.Accompanying meta.csv files provide summary volume-level metadata for each collection. For full details of methods and data format, see the interim project report at (http://dx.doi.org/10.6084/m9.figshare.1281251). For software and training data used in the project, see the repository (https://github.com/tedunderwood/genre).},
    publisher = {figshare},
    author = {Underwood, Ted},
    month = dec,
    year = {2014},
    doi = {10.6084/m9.figshare.1279201},
    keywords = {digital libraries, drama, fiction, genre, machine learning, metadata, poetry},
}
@incollection{belsham_stile_1799,
    address = {London},
    title = {On {Stile} and {Versification}},
    volume = {v.2},
    url = {https://prosody.princeton.edu/archive/njp.32101076530979-p482/},
    urldate = {2025-07-09},
    booktitle = {Essays philosophical and moral, historical and literary.},
    publisher = {G.G. and J. Robinson},
    author = {Belsham, William},
    year = {1799},
    pages = {482--513},
}
@misc{noauthor_journal_2025,
    title = {Journal of {Open} {Humanities} {Data}},
    url = {https://openhumanitiesdata.metajnl.com},
    language = {en},
    urldate = {2025-07-14},
    journal = {Journal of Open Humanities Data},
    month = jul,
    year = {2025},
}
@article{ries_reproducibility_2024,
    title = {Reproducibility and explainability in digital humanities},
    volume = {6},
    issn = {2524-7840},
    url = {https://doi.org/10.1007/s42803-023-00083-w},
    doi = {10.1007/s42803-023-00083-w},
    language = {en},
    number = {1},
    urldate = {2025-07-03},
    journal = {International Journal of Digital Humanities},
    author = {Ries, Thorsten and van Dalen-Oskam, Karina and Offert, Fabian},
    month = apr,
    year = {2024},
    keywords = {Digital humanities, Explainability, Methodology, Reproducibility},
    pages = {1--7},
}
@article{da_computational_2019,
    title = {The {Computational} {Case} against {Computational} {Literary} {Studies}},
    volume = {45},
    issn = {0093-1896},
    url = {https://www.journals.uchicago.edu/doi/10.1086/702594},
    doi = {10.1086/702594},
    number = {3},
    urldate = {2025-07-03},
    journal = {Critical Inquiry},
    author = {Da, Nan Z.},
    month = mar,
    year = {2019},
    note = {Publisher: The University of Chicago Press},
    pages = {601--639},
}
@article{burrows_reproducibility_2023,
    title = {Reproducibility, verifiability, and computational historical research},
    volume = {5},
    issn = {2524-7840},
    url = {https://doi.org/10.1007/s42803-023-00068-9},
    doi = {10.1007/s42803-023-00068-9},
    abstract = {Digital humanities methods have been at the heart of a recent series of high-profile historical research projects. But these approaches raise new questions about reproducibility and verifiability in a field of research where grounding one’s conclusions in a body of historical evidence is crucial. While there have been extensive debates about the nature and methods of historical research since the nineteenth century, the underlying assumption has generally been that documenting one’s sources in a series of footnotes is essential to enable other researchers to test the validity of the research. Even if this approach never amounted to “reproducibility” in the sense of scientific experimentation, it might still be seen as broadly analogous, since the evidence can be reassembled to see the basis for the explanations that were offered and to test their validity. This essay examines how new digital methods like topic modelling, network analysis, knowledge graphs, species models, and various kinds of visualizations are affecting the process of reproducing and verifying historical research. Using examples drawn from recent research projects, it identifies a need for thorough documentation and publication of the different layers of digital research: digital and digitized collections, descriptive metadata, the software used for analysis and visualizations, and the various settings and configurations.},
    language = {en},
    number = {2},
    urldate = {2025-07-03},
    journal = {International Journal of Digital Humanities},
    author = {Burrows, Toby},
    month = nov,
    year = {2023},
    keywords = {Archaeological Methodology, Archaeological Methods and Modelling, Digital Humanities, Digital history, Digital humanities, Ethnographic Methods, Historiography and Method, Reproducibility, Research Methods in Anthropology, Verifiability},
    pages = {283--298},
}